<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Compare your Bayesian Models: BIC</title>
  <meta name="description" content="Comparing Bayesian ModelsThere a number of model selection methods in machine learning. In essence, the methods try to balance training errors with the compl...">

  <!-- Google Fonts loaded here depending on setting in _data/options.yml true loads font, blank does not-->
  
    <link href='//fonts.googleapis.com/css?family=Lato:400,400italic' rel='stylesheet' type='text/css'>
  
  
  <!-- Load up MathJax script if needed ... specify in /_data/options.yml file-->
  
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  

  <link rel="stylesheet" type="text/css" href="/souravc83.github.io-blog/css/tufte.css">
  <!-- <link rel="stylesheet" type="text/css" href="/souravc83.github.io-blog/css/print.css" media="print"> -->

  <link rel="canonical" href="http://100.115.92.201:4000/souravc83.github.io-blog/articles/19/bayesian-info">

  <link rel="alternate" type="application/rss+xml" title="Tufte-Jekyll" href="http://100.115.92.201:4000/souravc83.github.io-blog/feed.xml" />
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
 (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
 m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
 })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

 ga('create', 'UA-30506809-1', 'auto');
 ga('send', 'pageview');

</script>

</head>

  <body>
    <!--- Header and nav template site-wide -->
<header>
    <nav class="group">
	<a href="/souravc83.github.io-blog/"><img class="badge" src="/souravc83.github.io-blog/assets/img/badge_1.png" alt="CH"></a>
	
		
  	
		
		    
		      <a href="/souravc83.github.io-blog/">blog</a>
		    
	    
  	
		
		    
		      <a href="/souravc83.github.io-blog/about/">About</a>
		    
	    
  	
		
		    
		      <a href="/souravc83.github.io-blog/page/">page</a>
		    
	    
  	
		
		    
		      <a href="/souravc83.github.io-blog/css/print.css"></a>
		    
	    
  	
		
  	
	</nav>
</header>

    <article class="group">
      <h1>Compare your Bayesian Models: BIC</h1>
<p class="subtitle">August 14, 2019</p>

<h2 id="comparing-bayesian-models">Comparing Bayesian Models</h2>

<p>There a number of model selection methods in machine learning. In essence, the methods try to balance training errors with the complexity of the model. In this post, we want to look at the Bayesian Information Criterion. Although its final approximate form is used widely in practice, there is less emphasis on its actual theoretical basis. This is what we will try to address in this post.</p>

<p>Given different Bayesian models, how can we compare the different models with each other. As an example, think of the Gaussian mixture model. Let’s say I am trying to fit the data I have with a mixture of K Gaussians. Different values of K correspond to different models. This is kinda equivalent to the question “how to choose K in K-means?”</p>

<p>More formally, let’s say we have a Bayesian model <span>​<script type="math/tex">M_i</script></span>, parameterized by the parameters <span>​<script type="math/tex">\theta_i</script></span>. We will start the analysis with a prior belief about <span>​<script type="math/tex">\theta_i</script></span> and then find update the belief after looking at the data.</p>

<p>We might ask, what is the probability of the data we observe conditioned on this model.</p>
<div class="mathblock"><script type="math/tex; mode=display">p(D) = \int p(D,\theta_i)d\theta_i = \int p(D|\theta_i)p(\theta_i)d\theta_i</script></div>

<p>The Bayesian information criterion tries to maximize this probability. In plain English, it is saying that the best model is the one, that makes the data most likely. In other words, among all models, this looks like the model from which the data was drawn from most likely.</p>

<p>Now, this integral above is hard to evaluate. So, we will need to resort to an approximation. Let us first take a detour, and learn about Laplace Approximation.</p>

<h2 id="laplace-approximation">Laplace Approximation</h2>

<p>Laplace approximation is a way of saying, if there is a complicated probability distribution we cannot find, we approximate it by a Gaussian, whose mean is at the mode of the distribution. The only other parameter we need to define is the variance. To define the variance, we will look at the Taylor expanstion of the function around its mode. We will show the result for the Taylor expansion for a scalar <span>​<script type="math/tex">z</script></span> and then we will show the extension when <span>​<script type="math/tex">z</script></span> is vector.</p>

<p><label for="mf-id-whatever" class="margin-toggle">⊕</label><input type="checkbox" id="mf-id-whatever" class="margin-toggle" /><span class="marginnote"><img class="fullwidth" src="/souravc83.github.io-blog/assets/img/test.png" /><br />Laplace Approximation basically approximates a complicated probability distribution with a Gaussian centered at the mode of the distribution</span></p>

<p>Now, lets see this mathematically. Lets say there is a probability density function</p>
<div class="mathblock"><script type="math/tex; mode=display">p(z) = \frac{1}{Z}f(z)</script></div>
<p>where <span>​<script type="math/tex">Z = \int f(z)dz</script></span>, which as we know, is in general, a difficult integral to compute. But since we have decided to approximate this with a Gaussian, and we know a Gaussian is a valid probability distribution, the Laplace approximation provides us an easy way to approximate this integral.</p>

<p>Now, lets consider a point <span>​<script type="math/tex">z_0</script></span> which is the mode. Therefore, by definition, <span>​<script type="math/tex">f'(z_0)= 0</script></span>. If we expand <span>​<script type="math/tex">ln(f(z))</script></span> in a taylor expansion around <span>​<script type="math/tex">z_0</script></span>, we get:</p>
<div class="mathblock"><script type="math/tex; mode=display">ln(f(z)) = ln(f(z_0)) + \frac{d}{dz}ln(f(z))|_{z_0}(z-z_0) + \frac{d^2}{d^2z}ln(f(z)|_{z_0}(z-z_0)^2 + ...</script></div>

<p>Notice that the second term will be zero, because</p>
<div class="mathblock"><script type="math/tex; mode=display">\frac{d}{dz} ln(f(z))|_{z_0} = \frac{1}{f(z_0)} f'(z_0) = 0</script></div>

<p>Therefore, we can write</p>
<div class="mathblock"><script type="math/tex; mode=display">ln(f(z)) = ln(f(z_0)) - \frac{1}{2}A(z-z_0)^2</script></div>

<p>Here &lt;div class="mathblock"&gt;<script type="math/tex; mode=display">A = - \frac{d^2}{dz^2} ln(f(z))|_{z_0}</script>&lt;/div&gt;</p>

<p>Taking exponentials</p>
<div class="mathblock"><script type="math/tex; mode=display">f(z) = f(z_0)exp(-\frac{A }{2}(z - z_0)^2)</script></div>

<p>Now, recognize that if this were a Gaussian, then <span>​<script type="math/tex">A = \frac{1}{\sigma^2}</script></span>.</p>

<p>Now, in the multivariate case, you can go through a similar derivation. The only difference is, <span>​<script type="math/tex">A</script></span> will be the Hessian matrix, instead of just being a single second derivative. This will correspond to the precision matrix of the multivariate normal, i.e. the inverse of the covariance matrix. If <span>​<script type="math/tex">z</script></span> is a vector, we can approximate the function <span>​<script type="math/tex">f(z)</script></span> as a multivariate normal, around its mode</p>

<div class="mathblock"><script type="math/tex; mode=display">f(z) =  f(z_0)exp(-\frac{1}{2}(z-z_0)^T A (z-z_0))</script></div>

<p>What this tells us, is that by ignoring the higher order terms of the Taylor expansion, we have managed to express an arbitrary function f(z) near its mode with a normal distribution, whose mean is the mode of the function, and precision matrix is the Hessian at <span>​<script type="math/tex">z_0</script></span>.</p>

<p>Repeating again what we said before,  <span>​<script type="math/tex">Z = \int f(z)dz</script></span> is difficult to calculate. Now, lets use the Laplace approximation to get it.</p>
<div class="mathblock"><script type="math/tex; mode=display">
\begin{align}
Z = \int f(z)dz = f(z_0)exp(-\frac{1}{2}(z-z_0)^T A (z-z_0)) dz \\ = f(z_0) \frac{(2\pi)^{M/2}}{|A|^{1/2}}\int \frac{|A|^{1/2}}{(2\pi)^{M/2}}exp(-\frac{1}{2}(z-z_0)^T A (z-z_0)) dz = f(z_0) \frac{(2\pi)^{M/2}}{|A|^{1/2}}
\end{align}</script></div>

<p>So, what happened here? In the first step, we wrote the expression we have derived before, for <span>​<script type="math/tex">f(z)</script></span>. Then we multiplied and divided the right hand side by <span>​<script type="math/tex">\frac{(2\pi)^{M/2}}{|A|^{1/2}}</script></span> and moved the integral to contain terms with z. Now, we multiplied and divided to make everything inside the integral sign equal to the density of the normal distribution, in M dimensions (A is the precision, i.e. the inverse of the covariance matrix). The density integrates to 1, and hence we are left with the term outside the integral.</p>

<p>In practice what this meant is that, to find this complicated integral, all that we need to do is to find the value of the function at <span>​<script type="math/tex">z_0</script></span> where the function reaches a maximum, and also the Hessian matrix A at <span>​<script type="math/tex">z_0</script></span>. Then we are all set to calculate the complicated integral we are after.</p>

<h2 id="back-to-bayesian-information-criterion">Back to Bayesian Information Criterion</h2>

<p>So, where we got stuck last time was that we did not know how to evaluate the integral to get the marginal. We wanted to evaluate:</p>
<div class="mathblock"><script type="math/tex; mode=display">p(D) = \int p(D,\theta_i)d\theta_i = \int p(D|\theta_i)p(\theta_i)d\theta_i</script></div>

<p>Here <span>​<script type="math/tex">f(\theta) = p(D|\theta)p(\theta)</script></span> and therefore, using Laplace approximation,</p>

<div class="mathblock"><script type="math/tex; mode=display">p(D) = p(D|\theta_{MAP})p(\theta_{MAP})\frac{(2\pi)^{M/2}}{|A|^{1/2}}</script></div>

<p>Taking log</p>
<div class="mathblock"><script type="math/tex; mode=display">ln(p(D)) = ln(p(D|\theta_{MAP}) + ln(p(\theta_{MAP})) + \frac{M}{2} ln(2\pi) - \frac{1}{2} ln|A|</script></div>

<p>Now, let’s assume that N is large, and terms that depend on N dominate this expression. 
It is easy to see that <span>​<script type="math/tex">ln(p(D|\theta_{MAP}) </script></span> grows with the size of the data. For example, if the data is IID, then</p>
<div class="mathblock"><script type="math/tex; mode=display">ln(p(D|\theta_{MAP}) = \Sigma_{i=1}^n ln(p(D_i|\theta_{MAP})</script></div>

<p>Similar to the likelihood, the hessian of the likelihood grows with the size of the data. Now, we will assume that the Hessian linearly increases with N, i.e. <span>​<script type="math/tex">A \approx N A_0</script></span>.
So, therefore, from the properties of determinants</p>
<div class="mathblock"><script type="math/tex; mode=display">|NA_0| = N^M A_0</script></div>
<p>if <span>​<script type="math/tex">A_0</script></span> is an M dimensional matrix. This is because imagine each term in the determinant is a product of m terms, each multiplied by N (in a 2D matrix [[a,b], [c,d]], the determinant is (ad - bc)).</p>

<p>We can ignore the terms that do not grow with the size of the data. These are the terms <span>​<script type="math/tex">ln(p(\theta_{MAP}))</script></span> and <span>​<script type="math/tex">\frac{M}{2} ln(2\pi)</script></span>.</p>

<p>Hence, we can now write:</p>
<div class="mathblock"><script type="math/tex; mode=display">
\begin{align}
ln(p(D)) \approx ln(p(D|\theta_{MAP}) - \frac{1}{2} ln|A|\\  =  ln(p(D|\theta_{MAP}) -   \frac{1}{2} lnN^M|A_0| \\ \approx ln(p(D|\theta_{MAP}) - \frac{M}{2} lnN
\end{align}
</script></div>

<p>This result defines the <strong>Bayesian information criterion</strong>. The Bayesian information criterion is important because it scores a model higher if the likelihood of the data given the posterior mode is high, but it penalizes a model that uses a large number of parameters(in this case M).</p>



    </article>
    <span class="print-footer">Compare your Bayesian Models: BIC - August 14, 2019 - clay harmon</span>
    <footer>
  <hr class="slender">
  <ul class="footer-links">
    <li><a href="mailto:hate@spam.net"><span class="icon-mail"></span></a></li>    
    
      <li>
        <a href="//www.twitter.com/souravc83"><span class="icon-twitter"></span></a>
      </li>
    
      <li>
        <a href="//github.com/souravc83"><span class="icon-github"></span></a>
      </li>
    
      <li>
        <a href="/feed"><span class="icon-feed"></span></a>
      </li>
      
  </ul>
<div class="credits">
<span>&copy; 2019 &nbsp;&nbsp;CLAY HARMON</span></br> <br>
<span>This site created with the <a href="//github.com/clayh53/tufte-jekyll">Tufte theme for Content-centric blogging </a> in <a href="//jekyllrb.com">Jekyll</a>.</span> 
</div>  
</footer>
  </body>
</html>
