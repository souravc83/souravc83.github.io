<!DOCTYPE html>
<html lang="en">

<head>
  <!-- ## for client-side less
  <link rel="stylesheet/less" type="text/css" href="./theme/css/style.less">
  <script src="http://cdnjs.cloudflare.com/ajax/libs/less.js/1.7.3/less.min.js" type="text/javascript"></script>
  -->
  <link rel="stylesheet" type="text/css" href="./theme/css/style.css">
  <link rel="stylesheet" type="text/css" href="./theme/css/pygments.css">
  <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=PT+Sans|PT+Serif|PT+Mono">

  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="author" content="Sourav Chatterjee">
  <meta name="description" content="Posts and writings by Sourav Chatterjee">

  <link href="http://souravc83.github.io/feeds/all.rss.xml" type="application/rss+xml" rel="alternate" title="Data.Coffee.Code() RSS" />

<meta name="keywords" content="">

  <title>
    Data.Coffee.Code()
&ndash; What's wrong with histograms and what are density estimators?  </title>


  <!-- Start of StatCounter Code for Default Guide -->
  <script type="text/javascript">
  var sc_project=('10783652');
  var sc_invisible=1;
  var sc_security=('24aa9956');
  var scJsHost = (("https:" == document.location.protocol) ?
  "https://secure." : "http://www.");
  document.write("<sc"+"ript type='text/javascript' src='" +
  scJsHost+
  "statcounter.com/counter/counter.js'></"+"script>");
  </script>
  <!-- End of StatCounter Code for Default Guide -->
</head>

<body>
  <aside>
    <div id="user_meta">
      <a href=".">
        <img src="/images/your_logo.png" alt="logo">
      </a>
      <h2><a href=".">Sourav Chatterjee</a></h2>
      <p></p>
      <ul>
        <li><a href="mailto:souravc83@gmail.com" target="_blank">Email</a></li>
        <li><a href="http://github.com/souravc83" target="_blank">Github</a></li>
        <li><a href="http://twitter.com/souravc83" target="_blank">Twitter</a></li>
        <li><a href="http://www.r-bloggers.com/" target="_blank">R Bloggers</a></li>
        <li><a href="http://andrewgelman.com/" target="_blank">Andrew Gelman</a></li>
        <li><a href="http://election.princeton.edu/" target="_blank">Sam Wang</a></li>
      </ul>
    </div>
  </aside>

  <main>
    <header>
      <p>
      <a href=".">Index</a> &brvbar; <a href="./archives.html">Archives</a>
      &brvbar; <a href="http://souravc83.github.io/feeds/all.rss.xml">RSS</a>
      </p>
    </header>

<article>
  <div class="article_title">
    <h3><a href="./whats-wrong-with-histograms-and-what-are-density-estimators.html">What's wrong with histograms and what are density estimators?</a></h3>
  </div>
  <div class="article_text">
    <h1>Estimating Probablity Densities</h1>
<p>The other day I was reading Parzen's density estimators in Bishop. Density estimators are a non-parametric way of estimating the underlying probability distributions, that might have generated the data 
that you are handed over. Parametric methods consist of assuming a distribution, and then estimating
the parameters of the distribution, that are most likely to have generated that data (MLE/MAP etc.).
Nonparametric estimations however, make no such assumptions.
The simplest non-parametric estimation? You have probably used it several times, without realizing it.
Its the humble histogram!</p>
<h2>Visualizing data: Histograms</h2>
<p>When we are given simple numerical data, I mean simple 1 Dimensional data, one of the first things we do is to look
 at how the data is distributed i.e. find its probability distribution. The simplest thing we can do is a histogram, but it might not be the best we can do.
 So, lets generate some data that comes from a mixture of two gaussians. We will look at the most hurriedly put together histogram and then refine it.</p>
<div class="highlight"><pre><span class="kp">set.seed</span><span class="p">(</span><span class="m">667</span><span class="p">)</span>
hist_data <span class="o">&lt;-</span> <span class="kt">c</span><span class="p">(</span> rnorm<span class="p">(</span><span class="m">100</span><span class="p">,</span><span class="m">1</span><span class="p">,</span><span class="m">1</span><span class="p">),</span> rnorm<span class="p">(</span><span class="m">100</span><span class="p">,</span><span class="m">3</span><span class="p">,</span><span class="m">2</span><span class="p">)</span> <span class="p">)</span>
hist_df <span class="o">&lt;-</span> <span class="kt">data.frame</span><span class="p">(</span>values<span class="o">=</span>hist_data<span class="p">)</span>
<span class="kn">library</span><span class="p">(</span>ggplot2<span class="p">)</span>
p_hist <span class="o">&lt;-</span> ggplot<span class="p">(</span>data<span class="o">=</span>hist_df<span class="p">)</span> <span class="o">+</span> geom_bar<span class="p">(</span>aes<span class="p">(</span>x<span class="o">=</span>values<span class="p">),</span>stat<span class="o">=</span><span class="s">&quot;bin&quot;</span><span class="p">)</span> <span class="o">+</span>
          labs<span class="p">(</span>x<span class="o">=</span><span class="s">&quot;Value&quot;</span><span class="p">,</span>y<span class="o">=</span><span class="s">&quot;Normalized Count&quot;</span><span class="p">)</span>
<span class="kp">print</span><span class="p">(</span>p_hist<span class="p">)</span>
</pre></div>


<div class="highlight"><pre>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
</pre></div>


<p><img alt="plot of chunk unnamed-chunk-1" src="./figure/density_estimate/unnamed-chunk-1-1.png" /></p>
<p>So, one thing needs to be corrected first. This is not a probability density function(pdf) because the condition for a pdf is explicitly:
$$ \int_{-\infty}^{\infty} p(x) = 1 $$</p>
<p>So, we need to normalize this histogram. But to normalize, we explicitly need to bin width. Let us assume a bin width of $w$, and lets say we have N data points. In our case, N is 200. So, What is the total area under the curve.For k bins, and a count of $C_i$ for each bin, we can write the area A as:
$$ A = \sum_{i=1}^{i=k}C_iw$$
$$ A = w\sum_{i=1}^{i=k}C_i = wN $$</p>
<p>Therefore the normalization factor has to be w*N, or the product of bin width and the total number of 
points. Let us now explicity set a bin width and normalize:</p>
<div class="highlight"><pre>w <span class="o">&lt;-</span><span class="m">0.2</span>
break_vec <span class="o">&lt;-</span> <span class="kp">seq</span><span class="p">(</span>from <span class="o">=</span> <span class="kp">min</span><span class="p">(</span>hist_data<span class="p">)</span><span class="o">-</span>w<span class="p">,</span> to<span class="o">=</span> <span class="kp">max</span><span class="p">(</span>hist_data<span class="p">)</span><span class="o">+</span>w<span class="p">,</span> by <span class="o">=</span> w<span class="p">)</span>
h1 <span class="o">&lt;-</span> hist<span class="p">(</span>hist_data<span class="p">,</span> breaks<span class="o">=</span> break_vec<span class="p">,</span> plot<span class="o">=</span><span class="bp">F</span><span class="p">)</span>
nhist_df <span class="o">&lt;-</span> <span class="kt">data.frame</span><span class="p">(</span>count<span class="o">=</span>h1<span class="o">$</span>counts<span class="p">,</span> xval <span class="o">=</span> h1<span class="o">$</span>mids<span class="p">)</span>
nhist_df<span class="o">$</span>count <span class="o">&lt;-</span> nhist_df<span class="o">$</span>count<span class="o">/</span><span class="p">(</span>w<span class="o">*</span><span class="kp">length</span><span class="p">(</span>hist_data<span class="p">))</span>
<span class="kp">print</span><span class="p">(</span><span class="kp">paste</span><span class="p">(</span><span class="s">&quot;Area under the curve is&quot;</span><span class="p">,</span><span class="kp">sum</span><span class="p">(</span>nhist_df<span class="o">$</span>count<span class="o">*</span>w<span class="p">)))</span>
</pre></div>


<div class="highlight"><pre>## [1] &quot;Area under the curve is 1&quot;
</pre></div>


<div class="highlight"><pre>p_norm <span class="o">&lt;-</span> ggplot<span class="p">(</span>nhist_df<span class="p">)</span> <span class="o">+</span> geom_bar<span class="p">(</span>aes<span class="p">(</span>x<span class="o">=</span>xval<span class="p">,</span> y<span class="o">=</span>count<span class="p">),</span> stat<span class="o">=</span><span class="s">&quot;identity&quot;</span><span class="p">)</span> <span class="o">+</span>
          labs<span class="p">(</span>x<span class="o">=</span><span class="s">&quot;Value&quot;</span><span class="p">,</span>y<span class="o">=</span><span class="s">&quot;Normalized Count&quot;</span><span class="p">)</span>
<span class="kp">print</span><span class="p">(</span>p_norm<span class="p">)</span>
</pre></div>


<p><img alt="plot of chunk unnamed-chunk-2" src="./figure/density_estimate/unnamed-chunk-2-1.png" /></p>
<p>That straightens out the issue about area under the curve. However, histograms aren't quite what we want. Why? because histograms show jumps which are not characateristic of the data but more characteristic of the finite sample size. We are looking to find the distribution that generated this data, and get around the finite sample size effect. </p>
<h2>Density Estimators</h2>
<p>To understand density estimators, let us first recreate our previous figure. How? we will not depend this time on R's built-in hist function, but make our own. Sounds like a rather fruitless exercise in coding up something rather trivial? Well, not quite. We will shortly see why.</p>
<div class="highlight"><pre>tophat_kernel <span class="o">&lt;-</span><span class="kr">function</span><span class="p">(</span>x<span class="p">,</span> x_n<span class="p">,</span> width<span class="p">)</span>
<span class="p">{</span>
  <span class="kr">if</span><span class="p">(</span> <span class="kp">abs</span><span class="p">(</span>x<span class="o">-</span>x_n<span class="p">)</span><span class="o">&lt;</span>width<span class="o">/</span><span class="m">2.</span> <span class="p">)</span>
    <span class="kr">return</span><span class="p">(</span><span class="m">1</span><span class="o">/</span>width<span class="p">)</span>
  <span class="kp">else</span>
    <span class="kr">return</span><span class="p">(</span><span class="m">0</span><span class="p">)</span>
<span class="p">}</span>

construct_hist <span class="o">&lt;-</span><span class="kr">function</span><span class="p">(</span>hist_data<span class="p">,</span> break_vec<span class="p">,</span>width<span class="p">,</span> kernel_function<span class="p">)</span>
<span class="p">{</span>
  mids_vec <span class="o">&lt;-</span> break_vec <span class="o">+</span> width<span class="o">/</span><span class="m">2</span>
  count_vec <span class="o">&lt;-</span> <span class="kp">rep</span><span class="p">(</span><span class="m">0</span><span class="p">,</span> <span class="kp">length</span><span class="p">(</span>mids_vec<span class="p">))</span>
  <span class="kr">for</span><span class="p">(</span> i <span class="kr">in</span> <span class="m">1</span><span class="o">:</span><span class="kp">length</span><span class="p">(</span>count_vec<span class="p">))</span>
  <span class="p">{</span>
    <span class="kr">for</span><span class="p">(</span>j <span class="kr">in</span> <span class="m">1</span><span class="o">:</span><span class="kp">length</span><span class="p">(</span>hist_data<span class="p">))</span>
    <span class="p">{</span>
      count_vec<span class="p">[</span>i<span class="p">]</span> <span class="o">&lt;-</span> count_vec<span class="p">[</span>i<span class="p">]</span> <span class="o">+</span> kernel_function<span class="p">(</span>hist_data<span class="p">[</span>j<span class="p">],</span>mids_vec<span class="p">[</span>i<span class="p">],</span> width<span class="p">)</span>
    <span class="p">}</span>
  <span class="p">}</span>
  nhist_df <span class="o">&lt;-</span> <span class="kt">data.frame</span><span class="p">(</span>count<span class="o">=</span>count_vec<span class="p">,</span> xval <span class="o">=</span> mids_vec<span class="p">)</span>
  nhist_df<span class="o">$</span>count <span class="o">&lt;-</span> nhist_df<span class="o">$</span>count<span class="o">/</span><span class="p">(</span><span class="kp">length</span><span class="p">(</span>hist_data<span class="p">))</span>
  <span class="kr">return</span><span class="p">(</span>nhist_df<span class="p">)</span>
<span class="p">}</span>

derived_nhist_df <span class="o">&lt;-</span>construct_hist<span class="p">(</span>hist_data<span class="p">,</span> break_vec<span class="p">,</span> w<span class="p">,</span> tophat_kernel<span class="p">)</span>
p_norm2 <span class="o">&lt;-</span> ggplot<span class="p">(</span>derived_nhist_df<span class="p">)</span> <span class="o">+</span> geom_bar<span class="p">(</span>aes<span class="p">(</span>x<span class="o">=</span>xval<span class="p">,</span> y<span class="o">=</span>count<span class="p">),</span> stat<span class="o">=</span><span class="s">&quot;identity&quot;</span><span class="p">)</span><span class="o">+</span>
            labs<span class="p">(</span>x<span class="o">=</span><span class="s">&quot;Value&quot;</span><span class="p">,</span>y<span class="o">=</span><span class="s">&quot;Normalized Count&quot;</span><span class="p">)</span>
<span class="kp">print</span><span class="p">(</span>p_norm2<span class="p">)</span>
</pre></div>


<p><img alt="plot of chunk unnamed-chunk-3" src="./figure/density_estimate/unnamed-chunk-3-1.png" /></p>
<p>Okay, so we have now introduced this double for loop, and made things run a lot slower. Why did we do this? to understand the thinking behind the histogram method of estimation. What we are trying to do here, is 
look at each of these bins (outer for loop), and for a single bin ask, for each data point (inner for loop) whether the data point contributes to the bin or not. By contribution what we mean here, is whether the data point falls in the width of the bin we have defined.
Now, notice that the sudden discontinuities arise from the way we are defining the contribution of a data point. Its a binary decision, either a data point contributes or not. However, now that we have defined this framework, we are free to change this form of the contribution. We can, for example, think of a Gaussian, where the contribution is not 0/1 but decreases exponentially from the center of the bin.
This is the basic argument of density estimation. Now, we need not stop at the Gaussian kernel, but 
substitute it with a exponential, cosine or any other kernel that makes sense. The Gaussian is the most
common kernel, so let us see what happens when we shift to it.</p>
<div class="highlight"><pre>gaussian_kernel <span class="o">&lt;-</span> <span class="kr">function</span><span class="p">(</span>x<span class="p">,</span>x_n<span class="p">,</span> width<span class="p">)</span>
<span class="p">{</span>
  norm_val <span class="o">=</span> <span class="p">(</span> <span class="m">1.</span><span class="o">/</span><span class="kp">sqrt</span><span class="p">(</span><span class="m">2.</span><span class="o">*</span><span class="kc">pi</span><span class="o">*</span>width<span class="o">*</span>width<span class="p">)</span> <span class="p">)</span><span class="o">*</span><span class="kp">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span>x<span class="o">-</span>x_n<span class="p">)</span><span class="o">^</span><span class="m">2</span><span class="o">/</span><span class="p">(</span><span class="m">2</span><span class="o">*</span>width<span class="o">^</span><span class="m">2</span><span class="p">))</span>
  <span class="kr">return</span><span class="p">(</span>norm_val<span class="p">)</span>
<span class="p">}</span>
gauss_hist_df <span class="o">&lt;-</span>construct_hist<span class="p">(</span>hist_data<span class="p">,</span> break_vec<span class="p">,</span>w<span class="p">,</span> gaussian_kernel<span class="p">)</span>
p_gauss <span class="o">&lt;-</span> ggplot<span class="p">(</span>gauss_hist_df<span class="p">)</span> <span class="o">+</span> geom_bar<span class="p">(</span>aes<span class="p">(</span>x<span class="o">=</span>xval<span class="p">,</span> y<span class="o">=</span>count<span class="p">),</span> stat<span class="o">=</span><span class="s">&quot;identity&quot;</span><span class="p">)</span> <span class="o">+</span>
            labs<span class="p">(</span>x<span class="o">=</span><span class="s">&quot;Value&quot;</span><span class="p">,</span>y<span class="o">=</span><span class="s">&quot;Normalized Count&quot;</span><span class="p">)</span>
<span class="kp">print</span><span class="p">(</span>p_gauss<span class="p">)</span>
</pre></div>


<p><img alt="plot of chunk unnamed-chunk-4" src="./figure/density_estimate/unnamed-chunk-4-1.png" />
On comparing the histogram(i.e. the tophat kernel with the Gaussian one) we can see
that the Gaussian has done a nice job of smoothing the kinks in the histogram.</p>
<div class="highlight"><pre>p_comp <span class="o">&lt;-</span> ggplot<span class="p">()</span> <span class="o">+</span> geom_bar<span class="p">(</span>data <span class="o">=</span> derived_nhist_df<span class="p">,</span> aes<span class="p">(</span>x<span class="o">=</span>xval<span class="p">,</span> y<span class="o">=</span>count<span class="p">),</span> stat<span class="o">=</span><span class="s">&quot;identity&quot;</span><span class="p">)</span> <span class="o">+</span>
           geom_line<span class="p">(</span>data<span class="o">=</span>gauss_hist_df<span class="p">,</span> aes<span class="p">(</span>x<span class="o">=</span>xval<span class="p">,</span>y<span class="o">=</span>count<span class="p">),</span> colour<span class="o">=</span><span class="s">&quot;blue&quot;</span><span class="p">,</span> size<span class="o">=</span><span class="m">2</span><span class="p">)</span> <span class="o">+</span>
           labs<span class="p">(</span>x<span class="o">=</span><span class="s">&quot;Value&quot;</span><span class="p">,</span>y<span class="o">=</span><span class="s">&quot;Normalized Count&quot;</span><span class="p">)</span>
<span class="kp">print</span><span class="p">(</span>p_comp<span class="p">)</span>
</pre></div>


<p><img alt="plot of chunk unnamed-chunk-5" src="./figure/density_estimate/unnamed-chunk-5-1.png" /></p>
<p>As you can see we have managed to consirably smooth the probability density, and now we can clearly
see the outline of the more tightly distributed Gaussian, centered around 1. Why is this practically important? This gets rid of artificial kinks in the probability density. Any estimations, based on the probability density, which assumes a nicely behaved function can now be applied more easily. For example, a derivative operator will be much more well behaved, and not show the sudden jumps due to artificial kinks in the histogram. </p>
<p>How well behaved this function is, is highly dependent on the bin width we choose, just like the histogram method is.
We have overlooked this question till now. We will leave the reader with an illustration of how the 
histogram changes with bin width.</p>
<div class="highlight"><pre>gauss_hist_w1 <span class="o">&lt;-</span>construct_hist<span class="p">(</span>hist_data<span class="p">,</span> break_vec<span class="p">,</span><span class="m">0.1</span><span class="p">,</span> gaussian_kernel<span class="p">)</span>
p_1 <span class="o">&lt;-</span> ggplot<span class="p">(</span>gauss_hist_w1<span class="p">)</span> <span class="o">+</span> geom_bar<span class="p">(</span>aes<span class="p">(</span>x<span class="o">=</span>xval<span class="p">,</span> y<span class="o">=</span>count<span class="p">),</span> stat<span class="o">=</span><span class="s">&quot;identity&quot;</span><span class="p">)</span> <span class="o">+</span>
      labs<span class="p">(</span>x<span class="o">=</span><span class="s">&quot;Value&quot;</span><span class="p">,</span>y<span class="o">=</span><span class="s">&quot;Normalized Count&quot;</span><span class="p">)</span>
gauss_hist_w3 <span class="o">&lt;-</span>construct_hist<span class="p">(</span>hist_data<span class="p">,</span> break_vec<span class="p">,</span><span class="m">0.5</span><span class="p">,</span> gaussian_kernel<span class="p">)</span> 

p_3 <span class="o">&lt;-</span> ggplot<span class="p">(</span>gauss_hist_w3<span class="p">)</span> <span class="o">+</span> geom_bar<span class="p">(</span>aes<span class="p">(</span>x<span class="o">=</span>xval<span class="p">,</span> y<span class="o">=</span>count<span class="p">),</span> stat<span class="o">=</span><span class="s">&quot;identity&quot;</span><span class="p">)</span> <span class="o">+</span>
      labs<span class="p">(</span>x<span class="o">=</span><span class="s">&quot;Value&quot;</span><span class="p">,</span>y<span class="o">=</span><span class="s">&quot;Normalized Count&quot;</span><span class="p">)</span>
gauss_hist_w4 <span class="o">&lt;-</span>construct_hist<span class="p">(</span>hist_data<span class="p">,</span> break_vec<span class="p">,</span><span class="m">2</span><span class="p">,</span> gaussian_kernel<span class="p">)</span>
p_4 <span class="o">&lt;-</span> ggplot<span class="p">(</span>gauss_hist_w4<span class="p">)</span> <span class="o">+</span> geom_bar<span class="p">(</span>aes<span class="p">(</span>x<span class="o">=</span>xval<span class="p">,</span> y<span class="o">=</span>count<span class="p">),</span> stat<span class="o">=</span><span class="s">&quot;identity&quot;</span><span class="p">)</span> <span class="o">+</span>
      labs<span class="p">(</span>x<span class="o">=</span><span class="s">&quot;Value&quot;</span><span class="p">,</span>y<span class="o">=</span><span class="s">&quot;Normalized Count&quot;</span><span class="p">)</span>
<span class="kn">library</span><span class="p">(</span>gridExtra<span class="p">)</span>
grid.arrange<span class="p">(</span>p_1<span class="p">,</span>p_gauss<span class="p">,</span> p_3<span class="p">,</span>p_4<span class="p">,</span> ncol<span class="o">=</span><span class="m">2</span><span class="p">)</span>
</pre></div>


<p><img alt="plot of chunk unnamed-chunk-6" src="./figure/density_estimate/unnamed-chunk-6-1.png" /></p>
  </div>
  <div class="article_meta">
    <p>Posted on: Sat 20 February 2016</p>
    <p>Category: <a href="./category/misc.html">misc</a>
    </p>
  </div>

  <div id="article_comments">
    <div id="disqus_thread"></div>
    <script type="text/javascript">
        var disqus_identifier = "whats-wrong-with-histograms-and-what-are-density-estimators.html";
        (function() {
             var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
             dsq.src = 'http://souravc83githubio.disqus.com/embed.js';
             (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
         })();
    </script>
  </div>

</article>


    <div id="ending_message">
      <p>&copy; Sourav Chatterjee. Built using <a href="http://getpelican.com" target="_blank">Pelican</a>. Theme by Giulio Fidente on <a href="https://github.com/gfidente/pelican-svbhack" target="_blank">github</a>. </p>
    </div>
  </main>
</body>
</html>
